{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# cell 1: Install dependencies\n",
        "!pip install -q faiss-cpu sentence-transformers bitsandbytes accelerate transformers requests beautifulsoup4\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "E2WlIN1_swWd"
      },
      "id": "E2WlIN1_swWd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 2: Import libraries\n",
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "Z4_3YDeMswaE"
      },
      "id": "Z4_3YDeMswaE",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 3: Define SimpleRAGSystem class - Combined\n",
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, embedding_model: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
        "                 llm_model_name: str = \"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.llm_model_name = llm_model_name\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "        self.setup_llm()\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize the language model with quantization\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.llm_model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    def load_documents(self, source: str, chunk_size: int = 500) -> List[str]:\n",
        "        \"\"\"Load text data from a URL or local file and split into chunks\"\"\"\n",
        "        print(\"Loading and processing documents...\")\n",
        "\n",
        "        text = \"\"\n",
        "\n",
        "        # If the source is a URL\n",
        "        if source.startswith(\"http\"):\n",
        "            response = requests.get(source)\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract only visible text\n",
        "            paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
        "            text = \"\\n\".join(paragraphs)\n",
        "        else:\n",
        "            # If it’s a local text file\n",
        "            with open(source, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "        # Basic cleaning\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        # Split text into chunks\n",
        "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "        self.documents = chunks\n",
        "        print(f\"✅ Loaded {len(chunks)} document chunks.\")\n",
        "        return chunks\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Create embeddings and build FAISS index\"\"\"\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"No documents loaded\")\n",
        "\n",
        "        self.embeddings = self.embedding_model.encode(self.documents)\n",
        "        self.embeddings = np.array(self.embeddings).astype('float32')\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call create_embeddings first.\")\n",
        "\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding).astype('float32')\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'content': self.documents[idx],\n",
        "                    'score': float(distance),\n",
        "                    'index': idx\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, query: str, context: str) -> str:\n",
        "        \"\"\"Generate answer using the LLM with context\"\"\"\n",
        "        prompt = f\"\"\"Based on the following context, please answer the question.\n",
        "If the context doesn't contain relevant information, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        response = self.generator(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Remove the prompt from the generated text\n",
        "        return response[0]['generated_text'].replace(prompt, '').strip()\n",
        "\n",
        "    def ask(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
        "        retrieved_docs = self.retrieve(query, k)\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
        "        answer = self.generate_answer(query, context)\n",
        "\n",
        "        return {\n",
        "            'question': query,\n",
        "            'answer': answer,\n",
        "            'sources': retrieved_docs,\n",
        "            'context': context\n",
        "        }\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save the RAG system\"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        # Save documents\n",
        "        with open(os.path.join(path, 'documents.json'), 'w') as f:\n",
        "            json.dump(self.documents, f)\n",
        "\n",
        "        # Save embeddings and index\n",
        "        if self.embeddings is not None:\n",
        "            np.save(os.path.join(path, 'embeddings.npy'), self.embeddings)\n",
        "            faiss.write_index(self.index, os.path.join(path, 'faiss.index'))\n",
        "\n",
        "    def load(self, path: str):\n",
        "        \"\"\"Load a saved RAG system\"\"\"\n",
        "        # Load documents\n",
        "        with open(os.path.join(path, 'documents.json'), 'r') as f:\n",
        "            self.documents = json.load(f)\n",
        "\n",
        "        # Load embeddings and index\n",
        "        self.embeddings = np.load(os.path.join(path, 'embeddings.npy'))\n",
        "        self.index = faiss.read_index(os.path.join(path, 'faiss.index'))"
      ],
      "metadata": {
        "id": "j0VidAEbswfa"
      },
      "id": "j0VidAEbswfa",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 4: Define SimpleRAGSystem class - Part 2 (LLM setup)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, llm_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        self.llm_model_name = llm_model_name\n",
        "        self.setup_llm()\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize the language model with quantization\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.llm_model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n"
      ],
      "metadata": {
        "id": "t2nvYEVtswg2"
      },
      "id": "t2nvYEVtswg2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 5: Define SimpleRAGSystem class - Part 3 (Document loading)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List\n",
        "\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, llm_model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        self.llm_model_name = llm_model_name\n",
        "        self.setup_llm()\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize the language model with quantization\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.llm_model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    def load_documents(self, source: str):\n",
        "        \"\"\"Load documents from URL or local file\"\"\"\n",
        "        if source.startswith('http'):\n",
        "            response = requests.get(source)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "            # Simple text cleaning\n",
        "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "            text = ' '.join(lines)\n",
        "            self.documents = self.split_text(text)\n",
        "        else:\n",
        "            with open(source, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "            self.documents = self.split_text(text)\n",
        "\n",
        "    def split_text(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            chunks.append(text[start:end])\n",
        "            start += chunk_size - chunk_overlap\n",
        "            if end >= len(text):\n",
        "                break\n",
        "        return chunks\n"
      ],
      "metadata": {
        "id": "LzeigUG8swif"
      },
      "id": "LzeigUG8swif",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 6: Define SimpleRAGSystem class - Part 4 (Embeddings and retrieval)\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Create embeddings and build FAISS index\"\"\"\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"No documents loaded\")\n",
        "\n",
        "        self.embeddings = self.embedding_model.encode(self.documents)\n",
        "        self.embeddings = np.array(self.embeddings).astype('float32')\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call create_embeddings first.\")\n",
        "\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding).astype('float32')\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'content': self.documents[idx],\n",
        "                    'score': float(distance),\n",
        "                    'index': idx\n",
        "                })\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "TVd8MJ22swwR"
      },
      "id": "TVd8MJ22swwR",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 7: Define SimpleRAGSystem class - Part 5 (Generation and main pipeline)\n",
        "from typing import Dict, Any\n",
        "\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, llm_model, tokenizer, generator, embedding_model):\n",
        "        self.model = llm_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.generator = generator\n",
        "        self.embedding_model = embedding_model\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "        self.index = None\n",
        "\n",
        "    def create_embeddings(self):\n",
        "        \"\"\"Create embeddings and build FAISS index\"\"\"\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"No documents loaded\")\n",
        "\n",
        "        self.embeddings = self.embedding_model.encode(self.documents)\n",
        "        self.embeddings = np.array(self.embeddings).astype('float32')\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call create_embeddings first.\")\n",
        "\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding).astype('float32')\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        for idx, distance in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'content': self.documents[idx],\n",
        "                    'score': float(distance),\n",
        "                    'index': idx\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, query: str, context: str) -> str:\n",
        "        \"\"\"Generate answer using the LLM with context\"\"\"\n",
        "        prompt = f\"\"\"Based on the following context, please answer the question.\n",
        "If the context doesn't contain relevant information, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        response = self.generator(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Remove the prompt from the generated text\n",
        "        return response[0]['generated_text'].replace(prompt, '').strip()\n",
        "\n",
        "    def ask(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
        "        retrieved_docs = self.retrieve(query, k)\n",
        "        context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
        "        answer = self.generate_answer(query, context)\n",
        "\n",
        "        return {\n",
        "            'question': query,\n",
        "            'answer': answer,\n",
        "            'sources': retrieved_docs,\n",
        "            'context': context\n",
        "        }\n"
      ],
      "metadata": {
        "id": "qJDczCKiswzk"
      },
      "id": "qJDczCKiswzk",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 8: Define SimpleRAGSystem class - Part 6 (Save/load functionality)\n",
        "\n",
        "def save(self, path: str):\n",
        "    \"\"\"Save the RAG system\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    # Save documents\n",
        "    with open(os.path.join(path, 'documents.json'), 'w') as f:\n",
        "        json.dump(self.documents, f)\n",
        "\n",
        "    # Save embeddings and index\n",
        "    if self.embeddings is not None:\n",
        "        np.save(os.path.join(path, 'embeddings.npy'), self.embeddings)\n",
        "        faiss.write_index(self.index, os.path.join(path, 'faiss.index'))\n",
        "\n",
        "def load(self, path: str):\n",
        "    \"\"\"Load a saved RAG system\"\"\"\n",
        "    # Load documents\n",
        "    with open(os.path.join(path, 'documents.json'), 'r') as f:\n",
        "        self.documents = json.load(f)\n",
        "\n",
        "    # Load embeddings and index\n",
        "    self.embeddings = np.load(os.path.join(path, 'embeddings.npy'))\n",
        "    self.index = faiss.read_index(os.path.join(path, 'faiss.index'))\n"
      ],
      "metadata": {
        "id": "HhFjeswksw1i"
      },
      "id": "HhFjeswksw1i",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 9: Initialize RAG system\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "print(\"Initializing RAG system...\")\n",
        "\n",
        "# Load the embedding model for document retrieval\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load the tokenizer and generator model (LLM)\n",
        "llm_model_name = \"tiiuae/falcon-7b-instruct\"  # You can replace with a smaller one if Colab RAM is low\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize SimpleRAGSystem with all components\n",
        "rag = SimpleRAGSystem(\n",
        "    llm_model_name=llm_model_name, # Pass model name for setup_llm\n",
        "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\" # Pass model name for __init__\n",
        ")\n",
        "\n",
        "# Assign the loaded model components to the rag object\n",
        "rag.tokenizer = tokenizer\n",
        "rag.model = model\n",
        "rag.generator = generator\n",
        "rag.embedding_model = embedding_model\n",
        "\n",
        "\n",
        "print(\"✅ RAG system initialized successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "f36d5d4630ec4c03b38e417747fa5635",
            "b92de202f9724a37995bead87f7a60cd",
            "8c231a872c924baeb61188f9556a031d",
            "61de3a3817204d718fdb69ed8de25b02",
            "70f86a86e5b545cbb836d8890e0fad11",
            "d9884a34770b47e6a1cf3b14350a83b9",
            "bd1f6e93f4d1408086037fad4c9de664",
            "81b1d69e7c404734936ee83af0f11955",
            "299b9405059e4d2b83c3bc3b905a11cd",
            "a044db46b6974a09969b79e50e98dec5",
            "267a6b34d9af4081be4d6e8dde9a6342"
          ]
        },
        "id": "5L0IyLBYsxCw",
        "outputId": "4e4aa1f7-2300-4600-df6f-ff16cc5d74d7"
      },
      "id": "5L0IyLBYsxCw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG system...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.tiiuae.falcon_hyphen_7b_hyphen_instruct.8782b5c5d8c9290412416618f36a133653e85285.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f36d5d4630ec4c03b38e417747fa5635"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 10: Define SimpleRAGSystem class - Part (Document Loading)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def load_documents(self, source: str, chunk_size: int = 500) -> List[str]:\n",
        "    \"\"\"Load text data from a URL or local file and split into chunks\"\"\"\n",
        "    print(\"Loading and processing documents...\")\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    # If the source is a URL\n",
        "    if source.startswith(\"http\"):\n",
        "        response = requests.get(source)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract only visible text\n",
        "        paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
        "        text = \"\\n\".join(paragraphs)\n",
        "    else:\n",
        "        # If it’s a local text file\n",
        "        with open(source, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "    # Basic cleaning\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    self.documents = chunks\n",
        "    print(f\"✅ Loaded {len(chunks)} document chunks.\")\n",
        "    return chunks\n",
        "\n",
        "# Attach this method to your class\n",
        "SimpleRAGSystem.load_documents = load_documents\n",
        "\n",
        "# Re-initialize the rag object to include the new method\n",
        "rag = SimpleRAGSystem(\n",
        "    llm_model=\"tiiuae/falcon-7b-instruct\",\n",
        "    tokenizer=tokenizer,\n",
        "    generator=generator,\n",
        "    embedding_model=embedding_model\n",
        ")"
      ],
      "metadata": {
        "id": "hbPjzernsxFm"
      },
      "id": "hbPjzernsxFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 11: Create embeddings\n",
        "print(\"Creating embeddings and FAISS index...\")\n",
        "# Load documents before creating embeddings\n",
        "rag.load_documents(\"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/generation/rag/rag-with-langchain/data/paul_graham_essay.txt\")\n",
        "rag.create_embeddings()\n",
        "print(\"Embeddings and index created successfully!\")"
      ],
      "metadata": {
        "id": "8Tl4ijf7y2Qd"
      },
      "id": "8Tl4ijf7y2Qd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 12: Test the system\n",
        "print(\"Testing the RAG system...\")\n",
        "questions = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"What are the main applications of AI?\",\n",
        "    \"How does machine learning relate to AI?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = rag.ask(question)\n",
        "    print(f\"Q: {result['question']}\")\n",
        "    print(f\"A: {result['answer']}\")\n",
        "    print(f\"Retrieved {len(result['sources'])} documents\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "_Co62GhTy2YJ"
      },
      "id": "_Co62GhTy2YJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 13: Save the system\n",
        "print(\"Saving the system...\")\n",
        "rag.save(\"saved_rag_system\")\n",
        "print(\"System saved to 'saved_rag_system'\")"
      ],
      "metadata": {
        "id": "0pyz0-Tp0erE"
      },
      "id": "0pyz0-Tp0erE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 14: Load and test saved system\n",
        "print(\"Testing saved system loading...\")\n",
        "new_rag = SimpleRAGSystem()\n",
        "new_rag.load(\"saved_rag_system\")\n",
        "\n",
        "test_result = new_rag.ask(\"What is machine learning?\")\n",
        "print(f\"Q: What is machine learning?\")\n",
        "print(f\"A: {test_result['answer']}\")\n",
        "print(\"System loaded and tested successfully!\")"
      ],
      "metadata": {
        "id": "3fHpgcy6y2ii"
      },
      "id": "3fHpgcy6y2ii",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f36d5d4630ec4c03b38e417747fa5635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b92de202f9724a37995bead87f7a60cd",
              "IPY_MODEL_8c231a872c924baeb61188f9556a031d",
              "IPY_MODEL_61de3a3817204d718fdb69ed8de25b02"
            ],
            "layout": "IPY_MODEL_70f86a86e5b545cbb836d8890e0fad11"
          }
        },
        "b92de202f9724a37995bead87f7a60cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9884a34770b47e6a1cf3b14350a83b9",
            "placeholder": "​",
            "style": "IPY_MODEL_bd1f6e93f4d1408086037fad4c9de664",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "8c231a872c924baeb61188f9556a031d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81b1d69e7c404734936ee83af0f11955",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_299b9405059e4d2b83c3bc3b905a11cd",
            "value": 0
          }
        },
        "61de3a3817204d718fdb69ed8de25b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a044db46b6974a09969b79e50e98dec5",
            "placeholder": "​",
            "style": "IPY_MODEL_267a6b34d9af4081be4d6e8dde9a6342",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "70f86a86e5b545cbb836d8890e0fad11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9884a34770b47e6a1cf3b14350a83b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd1f6e93f4d1408086037fad4c9de664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b1d69e7c404734936ee83af0f11955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299b9405059e4d2b83c3bc3b905a11cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a044db46b6974a09969b79e50e98dec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "267a6b34d9af4081be4d6e8dde9a6342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}